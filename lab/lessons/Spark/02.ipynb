{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347b347d-dd2a-40f6-a603-6d1bea073460",
   "metadata": {},
   "source": [
    "# Spark 2: Listen storage bucket for uploads with Spark\n",
    "\n",
    "In this lesson we modify the previous pyspark implementation so that it listens for uploaded files and automatically loads them to database.\n",
    "\n",
    "## Step 1: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3a99a-d013-46c6-82e6-204fe325ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import collections\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Load generic helper functions\n",
    "%run ../../common/jupyter.ipynb\n",
    "import src_common_database as db\n",
    "%run ../../common/spark.ipynb\n",
    "import src_common_util as util\n",
    "\n",
    "# Use storage bucket defined with environment variables\n",
    "bucket = os.environ['STORAGE_BUCKET_URL']\n",
    "protocol = st.init_spark(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9d9a7-1269-4577-a1f0-0c6b2ffe283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving data to database\n",
    "def save_data( rdd ):\n",
    "    if not rdd.isEmpty():\n",
    "        header = rdd.first()\n",
    "        df = rdd.filter(lambda row : row != header).toDF(header)\n",
    "        \n",
    "        # Determine table name and schema from the header\n",
    "        table_name = None\n",
    "        db_fd = None\n",
    "        if collections.Counter(header) == collections.Counter([\"Date\", \"Product\", \"Order\", \"Quantity\", \"Price\"]):\n",
    "            # Change dataframe schema to match the load_sales database view and\n",
    "            # generate unique key by concatenating order number and product SKU\n",
    "            table_name = \"load_sales\"\n",
    "            db_df = df.select(\n",
    "                concat(col(\"Order\"), lit(\".\"), col(\"Product\")).alias(\"key\"),\n",
    "                col(\"Date\").alias(\"date_key\"),\n",
    "                col(\"Product\").alias(\"product_key\"),\n",
    "                col(\"Order\").alias(\"order_number\"),\n",
    "                col(\"Quantity\").alias(\"quantity\").cast(IntegerType()),\n",
    "                col(\"Price\").alias(\"price\").cast(FloatType())\n",
    "            );\n",
    "\n",
    "        if table_name:\n",
    "            # Write the data to database\n",
    "            db_df.write.mode(\"append\").jdbc(db.get_jdbc_url(), table_name, properties=db.get_jdbc_options())\n",
    "            # DEBUG: Show saved data\n",
    "            print(\"INFO: Saved data to database table \" + table_name)\n",
    "            db_df.show()\n",
    "        else:\n",
    "            print(\"ERROR: Could not parse file with header: \" + ','.join(header), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc467de4-e21e-4bd5-98d4-6d82b7c31c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set streaming context with 10 second interval\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# Read new files from bucket as a stream and parse each non-empty CSV row into a list of whitespace-trimmed values\n",
    "# NOTE: textFileStream does not support pathGlobFilter or recursiveFileLookup\n",
    "stream_data = ssc.textFileStream(\n",
    "    protocol + bucket + \"/sales\"\n",
    ").filter(None).map( lambda x: [value.strip() for value in x.split(',')] )\n",
    "\n",
    "# Save data to database\n",
    "stream_data.foreachRDD(save_data)\n",
    "\n",
    "# Start streaming until terminated by user\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd701315-c0d6-4321-8744-dc3ea9fa4f91",
   "metadata": {},
   "source": [
    "## Step 2: Add a file to the storage bucket\n",
    "\n",
    "- Execute `taito open bucket` on command-line to open the locally running bucket on web browser.\n",
    "  - TIP: You can alternatively use `taito open bucket:ENV` to connect to a non-local bucket (ENV is `dev`, `test`, `stag`, or `prod`).\n",
    "- Sign in with access key `minio` and secret key `secret1234`.\n",
    "- Create a folder named `sales` and upload the `Sales.csv` file to the folder. If the file already exists, just overwrite it.\n",
    "- In a few seconds you should see a notification that new data was saved to the database.\n",
    "- Stop the execution by pressing stop button on the Jupyter Lab web user interface. You can ignore the `KeyboardInterrupt` error message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655e520-4723-432b-8d7e-7cacaca0bea1",
   "metadata": {},
   "source": [
    "## Next lesson: [Spark 3 - Analyze data with Spark](03.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16323ee6-e130-4f1c-b966-7c7f036a6a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
