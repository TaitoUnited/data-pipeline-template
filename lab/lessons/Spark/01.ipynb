{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f50370-ab3d-4cdc-8da7-acd0b390a494",
   "metadata": {},
   "source": [
    "# Spark 1: Extract, transform, and load a CSV file with Spark (from bucket to database)\n",
    "\n",
    "In this lesson we use PySpark to load some sales rows from CSV files located in a storage bucket and save them to `fact_sales` database table in PostgreSQL.\n",
    "\n",
    "## Step 1: Add a file to the storage bucket\n",
    "\n",
    "- Execute `taito open bucket` on command-line to open the locally running bucket on web browser.\n",
    "  - TIP: You can alternatively use `taito open bucket:ENV` to connect to a non-local bucket (ENV is `dev`, `test`, `stag`, or `prod`).\n",
    "- Login in with access key `minio` and secret key `secret1234`.\n",
    "- Create a folder named `sales` and upload the Sales.csv file to the folder.\n",
    "\n",
    "## Step 2: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c504a-bde0-44d2-beda-da9b051b7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Load generic helper functions\n",
    "%run ../../common/jupyter.ipynb\n",
    "import src_common_database as db\n",
    "%run ../../common/spark.ipynb\n",
    "import src_common_util as util\n",
    "\n",
    "# Use storage bucket defined with environment variables\n",
    "bucket = os.environ['STORAGE_BUCKET_URL']\n",
    "protocol = st.init_spark(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db612715-5e69-4267-946c-b23a2ad5a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files from the sales folder\n",
    "df = spark.read.csv(protocol + bucket + \"/sales\",        # Read from /sales folder\n",
    "                    pathGlobFilter=\"*.csv\",              # Read only *.csv files\n",
    "                    recursiveFileLookup=True,            # Read recursively also from subfolders\n",
    "                    modifiedAfter=\"2021-04-01T00:00:00\", # Fetch only files modified after this timestamp\n",
    "                    header=True,                         # Each CSV file includes a header row with column names\n",
    "                    ignoreLeadingWhiteSpace=True,        # Trim column values\n",
    "                    ignoreTrailingWhiteSpace=True,       # Trim column values\n",
    "                    mode=\"FAILFAST\")                     # Do not allow invalid CSV\n",
    "\n",
    "# DEBUG: Show the contents\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eb560-0ec6-4238-abaf-c762671ea209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe schema to match the database table and\n",
    "# generate unique key by concatenating order number and product SKU\n",
    "db_df = df.select(\n",
    "    concat(col(\"Order\"), lit(\".\"), col(\"Product\")).alias(\"key\"),\n",
    "    col(\"Date\").alias(\"date_key\"),\n",
    "    col(\"Product\").alias(\"product_key\"),\n",
    "    col(\"Order\").alias(\"order_number\"),\n",
    "    col(\"Quantity\").alias(\"quantity\").cast(IntegerType()),\n",
    "    col(\"Price\").alias(\"price\").cast(FloatType())\n",
    ");\n",
    "\n",
    "# DEBUG: Show the renamed schema\n",
    "db_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6760c8-c513-4ff8-a1d2-fee16437980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data to the fact_sales database table\n",
    "# NOTE: If you get \"ERROR: duplicate key value violates unique constraint\", execute `taito init --clean` to clean your database from old data.\n",
    "db_df.write.mode(\"append\").jdbc(db.get_jdbc_url(), \"fact_sales\", properties=db.get_jdbc_options())\n",
    "\n",
    "# DEBUG: Show the data stored in database\n",
    "spark.read.jdbc(db.get_jdbc_url(), \"fact_sales\", properties=db.get_jdbc_options()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5059149-f9f5-40c6-a7e7-5ef5f7c56bb6",
   "metadata": {},
   "source": [
    "## Step 3: Connect to the database with Taito CLI\n",
    "\n",
    "- Execute `taito db connect` on command-line to connect to the local database.\n",
    "  - TIP: You can alternatively use `taito db connect:ENV` to connect to a non-local database (ENV is `dev`, `test`, `stag`, or `prod`).\n",
    "- Show all sales rows with `select * from fact_sales`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc475f-6a6e-4a1c-b473-42f9b45c1083",
   "metadata": {},
   "source": [
    "## Step 4: Change the implementation to update existing data and insert new data\n",
    "\n",
    "Unfortunately Spark does not currently support upsert (see [SPARK-19335](https://issues.apache.org/jira/browse/SPARK-19335)). There are multiple ways to go around this, for example:\n",
    "\n",
    "- Write data to a separate loading view that has a trigger that executes upsert for the target table on insert.\n",
    "- Write data to a separate loading table that has a trigger that executes upsert for the target table on insert.\n",
    "- Write data to a temporary table and then merge the data to the target table with a custom sql clause.\n",
    "- Just overwrite all data in the target table, preferably with truncate mode to keep the table schema intact.\n",
    "\n",
    "This is how you can implement the first option (loading view). Normally we would add a new database migration for this with `taito db add NAME`, but since our database tables are not yet in production, we can just modify the existing migrations and redeploy them.\n",
    "\n",
    "TODO: UUSI MIGRAATIO NÄISTÄ!\n",
    "\n",
    "1. Copy-paste the following content to the existing files: `database/deploy/fact_sales.sql`, `database/revert/fact_sales.sql`, and `database/verify/fact_sales.sql`.\n",
    "\n",
    "```sql\n",
    "-- Deploy fact_sales to pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "CREATE TABLE fact_sales (\n",
    "  key text PRIMARY KEY,\n",
    "  date_key text NOT NULL REFERENCES dim_dates (key),\n",
    "  product_key text NOT NULL REFERENCES dim_products (key),\n",
    "  order_number text NOT NULL,\n",
    "  quantity integer NOT NULL,\n",
    "  price numeric(12,2) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE VIEW load_sales AS SELECT * FROM fact_sales;\n",
    "\n",
    "CREATE OR REPLACE FUNCTION load_sales() RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "  INSERT INTO fact_sales VALUES (NEW.*)\n",
    "  ON CONFLICT (key) DO\n",
    "    UPDATE SET\n",
    "      date_key = EXCLUDED.date_key,\n",
    "      product_key = EXCLUDED.product_key,\n",
    "      order_number = EXCLUDED.order_number,\n",
    "      quantity = EXCLUDED.quantity,\n",
    "      price = EXCLUDED.price;\n",
    "  RETURN new;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "CREATE TRIGGER load_sales\n",
    "INSTEAD OF INSERT ON load_sales\n",
    "FOR EACH ROW EXECUTE PROCEDURE load_sales();\n",
    "\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Revert fact_sales from pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "DROP TRIGGER load_sales ON load_sales;\n",
    "DROP FUNCTION load_sales;\n",
    "DROP VIEW load_sales;\n",
    "DROP TABLE fact_sales;\n",
    "\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Verify fact_sales on pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "SELECT key FROM load_sales LIMIT 1;\n",
    "SELECT key FROM fact_sales LIMIT 1;\n",
    "\n",
    "ROLLBACK;\n",
    "```\n",
    "    \n",
    "2. Redeploy database migrations and example data to local database with `taito init --clean`.\n",
    "3. Execute the following code to load CSV data to database yet again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a28f8ce-e8d5-412b-9e2a-9ad1ac3efb67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-baa075c5296e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write the data to the \"load_sales\" view instead of \"fact_sales\" table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_jdbc_database_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"load_sales\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_jdbc_database_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# DEBUG: Show the data stored in fact_sales. You manual data changes should have been overwritten.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_jdbc_database_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fact_sales\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_jdbc_database_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Write the data to the \"load_sales\" view instead of \"fact_sales\" table\n",
    "db_df.write.mode(\"append\").jdbc(db.get_jdbc_url(), \"load_sales\", properties=db.get_jdbc_options())\n",
    "\n",
    "# DEBUG: Show the data stored in fact_sales. You manual data changes should have been overwritten.\n",
    "spark.read.jdbc(db.get_jdbc_url(), \"fact_sales\", properties=db.get_jdbc_options()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c60ed-fc47-491f-a202-775fd48b6df6",
   "metadata": {},
   "source": [
    "4. Connect to the database with `taito db connect` and modify some quantity and price values manually in the `fact_sales` table. Also delete one of the rows.\n",
    "5. Execute the following code to make sure your manual changes will be overwritten on data load. Note that the CSV data contains only 4 rows (orders 00000000003, 00000000004, and 00000000005). Other rows wont be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d77522-606f-43e2-9d9f-a7cfe3265423",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-baa075c5296e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write the data to the \"load_sales\" view instead of \"fact_sales\" table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_jdbc_database_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"load_sales\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_jdbc_database_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# DEBUG: Show the data stored in fact_sales. You manual data changes should have been overwritten.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_jdbc_database_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fact_sales\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_jdbc_database_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Write the data to the \"load_sales\" view instead of \"fact_sales\" table\n",
    "db_df.write.mode(\"append\").jdbc(db.get_jdbc_url(), \"load_sales\", properties=db.get_jdbc_options())\n",
    "\n",
    "# DEBUG: Show the data stored in fact_sales. You manual data changes should have been overwritten.\n",
    "spark.read.jdbc(db.get_jdbc_url(), \"fact_sales\", properties=db.get_jdbc_options()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598d59e-032e-4359-ab93-b73cfedf0643",
   "metadata": {},
   "source": [
    "## Next lesson: [Spark 2 - Listen storage bucket for uploads with Spark](02.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011632c1-2d14-4d66-8696-d44211d3df03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
