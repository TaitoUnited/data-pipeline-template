{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb3063a-feaa-4a77-952f-3b2aa48cea7c",
   "metadata": {},
   "source": [
    "# Basics 3: Add a new dimension table to database and create data pipeline for it\n",
    "\n",
    "In this lesson we add a new dimension table to our data model. The new **dim_stores** dimension describes the store where the sale was made. It also contains location information like postal code, region, city, and country. We could model location as a separate dimension, as it would be more reusable that way, but this time we choose to include location attributes directly in the **dim_stores** dimension.\n",
    "\n",
    "## Step 1: Add a new database migration\n",
    "\n",
    "1. Execute `taito db add dim_stores`.\n",
    "2. Add the following content to the newly created files (**database/deploy/dim_stores.sql**, **database/revert/dim_stores.sql**, and **database/verify/dim_stores.sql**).\n",
    "\n",
    "```sql\n",
    "-- Deploy dim_stores to pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "CREATE TABLE dim_stores (\n",
    "  key text PRIMARY KEY,\n",
    "  name text NOT NULL,\n",
    "  postal_code text NOT NULL,\n",
    "  city text NOT NULL,\n",
    "  country text NOT NULL\n",
    ");\n",
    "\n",
    "CREATE VIEW load_stores AS SELECT * FROM dim_stores;\n",
    "\n",
    "CREATE OR REPLACE FUNCTION load_stores() RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "  INSERT INTO dim_stores VALUES (NEW.*)\n",
    "  ON CONFLICT (key) DO\n",
    "    UPDATE SET\n",
    "      name = EXCLUDED.name,\n",
    "      postal_code = EXCLUDED.postal_code,\n",
    "      city = EXCLUDED.city,\n",
    "      country = EXCLUDED.country;\n",
    "  RETURN new;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "CREATE TRIGGER load_stores\n",
    "INSTEAD OF INSERT ON load_stores\n",
    "FOR EACH ROW EXECUTE PROCEDURE load_stores();\n",
    "\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Revert dim_stores from pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "DROP TRIGGER load_stores ON load_stores;\n",
    "DROP FUNCTION load_stores;\n",
    "DROP VIEW load_stores;\n",
    "DROP TABLE dim_stores;\n",
    "\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Verify dim_stores on pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "SELECT key FROM load_stores LIMIT 1;\n",
    "SELECT key FROM dim_stores LIMIT 1;\n",
    "\n",
    "ROLLBACK;\n",
    "```\n",
    "\n",
    "3. Deploy the new database migration to local database with `taito db deploy`.\n",
    "\n",
    "## Step 2: Create a CSV files for districts and stores, and upload them to bucket\n",
    "\n",
    "1. Create district.csv file with the following content:\n",
    "\n",
    "```excel\n",
    "Postal Code,City,Country\n",
    "Unknown,Unknown,Unknown\n",
    "00100,Helsinki,Finland\n",
    "11122,Stockholm,Sweden\n",
    "```\n",
    "\n",
    "2. Create stores.csv file with the following content:\n",
    "\n",
    "```excel\n",
    "Name,Postal Code\n",
    "Unknown,Unknown\n",
    "Super Shop,00100\n",
    "Super Shop,11122\n",
    "```\n",
    "\n",
    "2. Upload both files to the root folder of the bucket\n",
    "\n",
    "## Step 3: Load the CSV file to database\n",
    "\n",
    "Execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e13da-c784-4935-9b10-7d20e63881e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Load generic helper functions\n",
    "%run ../../common/jupyter.ipynb\n",
    "import src_common_database as db\n",
    "import src_common_storage as st\n",
    "import src_common_util as util\n",
    "\n",
    "# Read CSV files from the storage bucket\n",
    "bucket = st.create_storage_bucket_client(os.environ['STORAGE_BUCKET'])\n",
    "districts_csv = bucket.get_object_contents(\"/districts.csv\")\n",
    "stores_csv = bucket.get_object_contents(\"/stores.csv\")\n",
    "\n",
    "# Read CSV data into a Pandas dataframe\n",
    "districts_df = pd.read_csv(districts_csv)\n",
    "stores_df = pd.read_csv(stores_csv)\n",
    "\n",
    "# Merge\n",
    "df = pd.merge(stores_df, districts_df, on=['Postal Code','Postal Code'])\n",
    "\n",
    "# Change dataframe schema to match the database table\n",
    "db_df = df.rename(\n",
    "    columns = {\n",
    "        'Name': 'name',\n",
    "        'Postal Code': 'postal_code',\n",
    "        'City': 'city',\n",
    "        'Country': 'country',\n",
    "    },\n",
    "    inplace = False\n",
    ")\n",
    "\n",
    "# Generate unique key by concatenating concatenating name and country\n",
    "db_df[\"key\"] = db_df[\"country\"] + \" - \" + db_df[\"name\"]\n",
    "\n",
    "# Write the data to the \"load_stores\" view\n",
    "database = db.create_engine()\n",
    "db_df.to_sql('load_stores', con=database, if_exists='append', index=False)\n",
    "\n",
    "# DEBUG: Show the data stored in fact_sales. You manual data changes should have been overwritten.\n",
    "pd.read_sql('select * from dim_stores', con=database).style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f7098-77bf-4175-b649-5fe813a8bb82",
   "metadata": {},
   "source": [
    "## Step 4: Add dim_store reference to the fact_sales table\n",
    "\n",
    "This time you cannot just add the new columns to the existing fact_sales migration files, because fact_sales migration was created before the dim_sales migration. However, if you want to avoid creating a new migration just for one new column, you can do the following:\n",
    "\n",
    "1. Move the dim_stores migration one step up in **database/sqitch.plan** so that it will be executed before fact_sales.\n",
    "\n",
    "2. Add the new store_key column to the **database/deploy/fact_sales.sql** file:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE fact_sales (\n",
    "  ...\n",
    "  store_key text NOT NULL REFERENCES dim_stores (key),\n",
    "  ...\n",
    ");\n",
    "```\n",
    "\n",
    "3. Add at least one example store to the **database/data/dev.sql** file. If the file contains fact_sales rows, add the store rows before them.\n",
    "\n",
    "4. If the file contains fact_sales rows, add a store_key value for each example sale defined in **database/data/dev.sql**. If the file does not contain any fact_sales rows, add at least couple of example rows.\n",
    "\n",
    "5. Redeploy all database migrations and example data with `taito init --clean`. Redeploy is required because you altered the sqitch.plan order instead of creating a new ALTER TABLE database migration.\n",
    "\n",
    "## Step 5 (optional): Generate database documentation\n",
    "\n",
    "1. Generate database documentation with `taito db generate`.\n",
    "\n",
    "2. Open the `docs/database/index.html` file with your web browser. Note that your code editor may not display these files as they have been placed in .gitignore.\n",
    "\n",
    "3. Browse to **Relationships**.\n",
    "\n",
    "As you can see, our database model is based on [star schema](https://en.wikipedia.org/wiki/Star_schema).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7feca6-e692-4195-a1c1-b4418990f643",
   "metadata": {},
   "source": [
    "## Next lesson: [Basics 4 - Create a dataset view on top of star schema](04.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1afc5d-4516-4fd1-84c4-2bb90f579696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
