{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb3063a-feaa-4a77-952f-3b2aa48cea7c",
   "metadata": {},
   "source": [
    "# Basics 1: Extract, transform, and load a single CSV file (from bucket to database)\n",
    "\n",
    "In this lesson we load some sales rows from a CSV file located in a storage bucket and save them to fact_sales database table in PostgreSQL.\n",
    "\n",
    "## Step 1: Add a file to the storage bucket\n",
    "\n",
    "1. Execute `taito open bucket` on command-line to open the storage bucket on your web browser.\n",
    "2. Login in with access key `minio` and secret key `secret1234`.\n",
    "3. Create a folder named `sales` and upload a file named `sales-2021-04.csv` in the folder with the following content:\n",
    "\n",
    "```excel\n",
    "Order,Date,Product,Quantity,Price\n",
    "00000000003,2021-04-15,1-1,2,2129.00\n",
    "00000000003,2021-04-15,1-2,1,2659.00\n",
    "00000000004,2021-04-16,1-1,1,2659.00\n",
    "00000000005,2021-04-16,1-1,1,2129.00\n",
    "```\n",
    "\n",
    "## Step 2: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e89e06-8603-48ca-9039-db6303e76760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%run ../../common/jupyter.ipynb\n",
    "import src_common_database as db\n",
    "import src_common_storage as st\n",
    "import src_common_util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbb3fe-cb6c-4147-b106-0dc1918d36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file from the storage bucket\n",
    "bucket = st.create_storage_bucket_client(os.environ['STORAGE_BUCKET'])\n",
    "sales_csv = bucket.get_object_contents(\"/sales/sales-2021-04.csv\")\n",
    "\n",
    "# Read Sales.csv data into a Pandas dataframe\n",
    "df = pd.read_csv(sales_csv)\n",
    "\n",
    "# DEBUG: Show the contents\n",
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64f766-dd1c-4664-8322-7daa5f6734a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe schema to match the database table\n",
    "db_df = df.rename(\n",
    "    columns = {\n",
    "        'Date': 'date_key',\n",
    "        'Product': 'product_key',\n",
    "        'Order': 'order_number',\n",
    "        'Quantity': 'quantity',\n",
    "        'Price': 'price',\n",
    "    },\n",
    "    inplace = False\n",
    ")\n",
    "\n",
    "# Generate unique key by concatenating order number and product SKU\n",
    "db_df[\"key\"] = db_df[\"order_number\"].astype(str) + \".\" + db_df[\"product_key\"]\n",
    "\n",
    "# DEBUG: Show the renamed schema\n",
    "db_df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab40082-5fec-4a8e-bd41-42fe07d68f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data to the fact_sales database table\n",
    "# NOTE: You will get \"duplicate key value violates unique constraint\" or \"null value in column store_key violates not-null constraint\" errors\n",
    "#       if you have already executed some of the later lessons. In such case you should remove all your changes from \"database/\" and execute\n",
    "#       `taito init --clean` to clean your database from old data.\n",
    "database = db.create_engine()\n",
    "db_df.to_sql('fact_sales', con=database, if_exists='append', index=False)\n",
    "\n",
    "# DEBUG: Show the data stored in database\n",
    "pd.read_sql('fact_sales', con=database).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21bb5f-1ec0-4a21-9e55-8cb41cd83447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIP: In a real world example you would probably list all CSV files from a folder\n",
    "# and execute the operation to each of them, for example:\n",
    "filenames = bucket.list_objects(\"/sales/\")\n",
    "for filename in filenames:\n",
    "    print(\"Executing operation for \" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af659f0-ea10-4126-8dd7-634114ae56c2",
   "metadata": {},
   "source": [
    "## Step 3: Connect to the database with Taito CLI\n",
    "\n",
    "- Execute `taito db connect` on command-line to connect to the local database.\n",
    "- Show all sales rows with `select * from fact_sales`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb3380-76ca-4170-9dfc-43aa4d6bb445",
   "metadata": {},
   "source": [
    "## Step 4: Change the implementation to update existing data and insert new data\n",
    "\n",
    "Currently our implementation only inserts new data to the database table and fails if there is existing data with the same unique key. Unfortunately Pandas does not support PostgreSQL upsert (insert or update). There are multiple ways to go around this, for example:\n",
    "\n",
    "- Write data to a separate loading view that has a trigger that executes upsert for the target table on insert.\n",
    "- Write data to a separate loading table that has a trigger that executes upsert for the target table on insert.\n",
    "- Write data to a temporary table and then merge the data to the target table with a custom sql clause.\n",
    "- Just overwrite all data in the target table, preferably with truncate mode to keep the table schema intact.\n",
    "\n",
    "This is how you can implement the first option (loading view). Normally we would add a new database migration for this, but since our database tables are not yet in production, we can just modify the existing migrations and redeploy them.\n",
    "\n",
    "1. Copy-paste the following content to the existing files: `database/deploy/fact_sales.sql`, `database/revert/fact_sales.sql`, and `database/verify/fact_sales.sql`.\n",
    "\n",
    "```sql\n",
    "-- Deploy fact_sales to pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "CREATE TABLE fact_sales (\n",
    "  key text PRIMARY KEY,\n",
    "  date_key text NOT NULL REFERENCES dim_dates (key),\n",
    "  product_key text NOT NULL REFERENCES dim_products (key),\n",
    "  order_number text NOT NULL,\n",
    "  quantity integer NOT NULL,\n",
    "  price numeric(12,2) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE VIEW load_sales AS SELECT * FROM fact_sales;\n",
    "\n",
    "CREATE OR REPLACE FUNCTION load_sales() RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "  INSERT INTO fact_sales VALUES (NEW.*)\n",
    "  ON CONFLICT (key) DO\n",
    "    UPDATE SET\n",
    "      date_key = EXCLUDED.date_key,\n",
    "      product_key = EXCLUDED.product_key,\n",
    "      order_number = EXCLUDED.order_number,\n",
    "      quantity = EXCLUDED.quantity,\n",
    "      price = EXCLUDED.price;\n",
    "  RETURN new;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "CREATE TRIGGER load_sales\n",
    "INSTEAD OF INSERT ON load_sales\n",
    "FOR EACH ROW EXECUTE PROCEDURE load_sales();\n",
    "\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Revert fact_sales from pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "DROP TRIGGER load_sales ON load_sales;\n",
    "DROP FUNCTION load_sales;\n",
    "DROP VIEW load_sales;\n",
    "DROP TABLE fact_sales;\n",
    "\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Verify fact_sales on pg\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "SELECT key FROM load_sales LIMIT 1;\n",
    "SELECT key FROM fact_sales LIMIT 1;\n",
    "\n",
    "ROLLBACK;\n",
    "```\n",
    "    \n",
    "2. Redeploy database migrations and example data to local database with `taito init --clean`.\n",
    "3. Execute the following code to load CSV data to database yet again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ac4b0-76e4-4dcc-9404-dacf99d7e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to the \"load_sales\" view instead of \"fact_sales\" table\n",
    "db_df.to_sql('load_sales', con=database, if_exists='append', index=False)\n",
    "\n",
    "# DEBUG: Show the data stored in fact_sales. You manual data changes should have been overwritten.\n",
    "pd.read_sql('fact_sales', con=database).style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf7382-f700-46d7-9a1e-a74d580a3e54",
   "metadata": {},
   "source": [
    "## Next lesson: [Basics 2 - Listen storage bucket for uploads](02.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1bd61-5d02-4083-97ff-c775ce04ff93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
